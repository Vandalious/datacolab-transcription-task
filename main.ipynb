{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arash Hajian nezhad | DataCoLab Interview Task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Add full text `body` and `start` / `end` times of the transcriptions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read transcriptions json files\n",
    "Insert necessary data into a dictionary for later usage in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions_path = 'data/transcriptions/'\n",
    "json_files = [json_file for json_file in os.listdir(transcriptions_path)]\n",
    "\n",
    "transcriptions = {}\n",
    "for json_file in json_files:\n",
    "    with open(os.path.join(transcriptions_path, json_file), 'r') as j:\n",
    "        source_video_id = json_file.split('.')[0]  # for removeing '.json' from the file name and leaving \n",
    "        current_data = json.loads(j.read())\n",
    "\n",
    "        transcriptions[source_video_id] = {'text_body': current_data['text'], 'words_data': current_data['words']}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the dataframe that needs to be filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_words</th>\n",
       "      <th>last_words</th>\n",
       "      <th>source_video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well knew. This morning police need your help</td>\n",
       "      <td>gunpoint, beating him and stealing his cell ph...</td>\n",
       "      <td>18246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a call. San Francisco firefighters rescued a man</td>\n",
       "      <td>all the way down to the ocean. ocean.</td>\n",
       "      <td>12387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paul. Meanwhile, the state set a record in</td>\n",
       "      <td>night through conservation, some 4000 conserva...</td>\n",
       "      <td>16859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emergency crews in Florida continue to search for</td>\n",
       "      <td>in Florida to more than 850,000 homes.</td>\n",
       "      <td>18246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But even though the state never ordered rolling</td>\n",
       "      <td>feel since their power got cut out needlessly.</td>\n",
       "      <td>16859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         first_words   \n",
       "0      Well knew. This morning police need your help  \\\n",
       "1   a call. San Francisco firefighters rescued a man   \n",
       "2         Paul. Meanwhile, the state set a record in   \n",
       "3  Emergency crews in Florida continue to search for   \n",
       "4    But even though the state never ordered rolling   \n",
       "\n",
       "                                          last_words  source_video_id  \n",
       "0  gunpoint, beating him and stealing his cell ph...            18246  \n",
       "1              all the way down to the ocean. ocean.            12387  \n",
       "2  night through conservation, some 4000 conserva...            16859  \n",
       "3            in Florida to more than 850,000 homes.             18246  \n",
       "4     feel since their power got cut out needlessly.            16859  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/to_fill.csv')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some data cleaning\n",
    "After some personal data exploration, I have noticed that some of the sentences in the `last_words` column have a reduntant empty `''` at their ends. These will prove to be problematic if left unaccounted for, hence the first thing we will clean from the data is this.\n",
    "\n",
    "Secondly, some sentences in the mentioned column end with a redunant word, which is not found anywhere in the transcriptions. This word is the same as its previous word in the sentence, for example `all the way down to the ocean. ocean.` which has an extra `ocean.` in the end. This is not found anywhere in the transcriptions and must be dealt with, which we do :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Function that takes strings and cleans them specifically for this task.\n",
    "    \n",
    "    Args:\n",
    "        text: a string that needs to be processed.\n",
    "    \n",
    "    Returns\n",
    "        returns a processed string.\n",
    "    \"\"\"\n",
    "    listed_text = text.split(' ')\n",
    "\n",
    "    if listed_text[-1] == '':\n",
    "        listed_text.pop()\n",
    "\n",
    "    if listed_text[-1] == listed_text[-2]:\n",
    "        listed_text.pop()\n",
    "    \n",
    "    return ' '.join(listed_text)\n",
    "\n",
    "\n",
    "df['last_words'] = df['last_words'].apply(data_cleaning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main functions\n",
    "These functions are used to do the main job of fetching the whole paragraph using the now-cleaned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_start_and_end_indices(text_body: list, first_words: list, last_words: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Function for finding the starting and ending indices of the paragraph\n",
    "    in the main text body using the `first_words` and `last_words` of it.\n",
    "\n",
    "    Args:\n",
    "        text_body: a list of strings made from the main text body.\n",
    "        first_words: a list of strings made from the `first_words` column.\n",
    "        last_words: a list of strings made from the `last_words` column.\n",
    "\n",
    "    Returns:\n",
    "        a tuple made of the starting and ending indices in the main text\n",
    "        body.\n",
    "    \"\"\"\n",
    "    possible_starts = [i for i, word in enumerate(text_body) if word == first_words[0]]\n",
    "\n",
    "    for starting_index in possible_starts:\n",
    "        if text_body[starting_index:starting_index + len(first_words)] == first_words:\n",
    "            break\n",
    "\n",
    "    possible_ends = [i + starting_index for i, word in enumerate(text_body[starting_index:]) if word == last_words[0]]\n",
    "\n",
    "    last_words_length = len(last_words)\n",
    "\n",
    "    for ending_index in possible_ends:\n",
    "        if text_body[ending_index:ending_index + last_words_length] == last_words:\n",
    "            break\n",
    "\n",
    "    return starting_index, ending_index + last_words_length  # last_words_length is added to return the end of the paragraph index.\n",
    "\n",
    "\n",
    "def get_full_text_and_times(row: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function for fetching the whole paragraph, start time and end time using\n",
    "    the `first_words` and `last_words` of it.\n",
    "\n",
    "    Args:\n",
    "        row: a row of a pandas dataframe.\n",
    "\n",
    "    Returns:\n",
    "        a pandas dataframe row cotaining three columns of `body`, `start` and `end`.\n",
    "    \"\"\"\n",
    "    # getting the body first\n",
    "    transcription_id = str(row['source_video_id'])\n",
    "    current_transcription = transcriptions[transcription_id]\n",
    "\n",
    "    listed_text_body = current_transcription['text_body'].split(' ')\n",
    "    listed_first_words = row['first_words'].split(' ')\n",
    "    listed_last_words = row['last_words'].split(' ')\n",
    "\n",
    "    start_index, end_index = find_start_and_end_indices(listed_text_body, listed_first_words, listed_last_words)\n",
    "\n",
    "    # now getting the start and end times\n",
    "    words_data = current_transcription['words_data']\n",
    "\n",
    "    start_time, end_time = words_data[start_index]['start'], words_data[end_index]['end']\n",
    "\n",
    "    # collating the fetched data\n",
    "    row['body'] = ' '.join(listed_text_body[start_index:end_index])\n",
    "    row['start'] = start_time\n",
    "    row['end'] = end_time\n",
    "\n",
    "    return row"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the functions\n",
    "Now we fetch the `body`, `start` and `end` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(get_full_text_and_times, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_words</th>\n",
       "      <th>last_words</th>\n",
       "      <th>source_video_id</th>\n",
       "      <th>body</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well knew. This morning police need your help</td>\n",
       "      <td>gunpoint, beating him and stealing his cell ph...</td>\n",
       "      <td>18246</td>\n",
       "      <td>Well knew. This morning police need your help ...</td>\n",
       "      <td>464928</td>\n",
       "      <td>505910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a call. San Francisco firefighters rescued a man</td>\n",
       "      <td>all the way down to the ocean.</td>\n",
       "      <td>12387</td>\n",
       "      <td>a call. San Francisco firefighters rescued a m...</td>\n",
       "      <td>359020</td>\n",
       "      <td>385526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paul. Meanwhile, the state set a record in</td>\n",
       "      <td>night through conservation, some 4000 conserva...</td>\n",
       "      <td>16859</td>\n",
       "      <td>Paul. Meanwhile, the state set a record in ene...</td>\n",
       "      <td>60704</td>\n",
       "      <td>101238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emergency crews in Florida continue to search for</td>\n",
       "      <td>in Florida to more than 850,000 homes.</td>\n",
       "      <td>18246</td>\n",
       "      <td>Emergency crews in Florida continue to search ...</td>\n",
       "      <td>505290</td>\n",
       "      <td>534958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But even though the state never ordered rolling</td>\n",
       "      <td>feel since their power got cut out needlessly.</td>\n",
       "      <td>16859</td>\n",
       "      <td>But even though the state never ordered rollin...</td>\n",
       "      <td>100910</td>\n",
       "      <td>283606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         first_words   \n",
       "0      Well knew. This morning police need your help  \\\n",
       "1   a call. San Francisco firefighters rescued a man   \n",
       "2         Paul. Meanwhile, the state set a record in   \n",
       "3  Emergency crews in Florida continue to search for   \n",
       "4    But even though the state never ordered rolling   \n",
       "\n",
       "                                          last_words  source_video_id   \n",
       "0  gunpoint, beating him and stealing his cell ph...            18246  \\\n",
       "1                     all the way down to the ocean.            12387   \n",
       "2  night through conservation, some 4000 conserva...            16859   \n",
       "3             in Florida to more than 850,000 homes.            18246   \n",
       "4     feel since their power got cut out needlessly.            16859   \n",
       "\n",
       "                                                body   start     end  \n",
       "0  Well knew. This morning police need your help ...  464928  505910  \n",
       "1  a call. San Francisco firefighters rescued a m...  359020  385526  \n",
       "2  Paul. Meanwhile, the state set a record in ene...   60704  101238  \n",
       "3  Emergency crews in Florida continue to search ...  505290  534958  \n",
       "4  But even though the state never ordered rollin...  100910  283606  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Predicting the `topics` of the paragraphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the `stories` dataframe\n",
    "This dataframe is used for training model(s) for predicting the `topics` of the now-fetched paragraphs\n",
    "\n",
    "Note: I have trained a BERT model for predicting the `topics` in another notebook (colab) which is\n",
    "included in the repo, but it has not yielded good results, as the `topics` labels are very sparse\n",
    "(many zeros and few ones), hence the model will always try to output zeros, as it lowers the error\n",
    "anyways. I have attempted to implement `positional weighting` technique for usage in BCEWithLogitsLoss\n",
    "but still no good results were observed, so I sticked with vanilla `TF-IDF` method and RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>['39822b5f-e37e-43e8-b997-7142fe55c3ea']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>['0d817400-3f5d-41e0-929c-c31fdbe75d31']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>['83a09c6b-5f2f-421f-ae50-b38acca7e008']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>['6fbf954a-03f9-4782-a65f-783271c9c447']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello and welcome to BBC News a woman who gave...</td>\n",
       "      <td>['83a09c6b-5f2f-421f-ae50-b38acca7e008', '9ff5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body   \n",
       "0                                                     \\\n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  hello and welcome to BBC News a woman who gave...   \n",
       "\n",
       "                                               topic  \n",
       "0           ['39822b5f-e37e-43e8-b997-7142fe55c3ea']  \n",
       "1           ['0d817400-3f5d-41e0-929c-c31fdbe75d31']  \n",
       "2           ['83a09c6b-5f2f-421f-ae50-b38acca7e008']  \n",
       "3           ['6fbf954a-03f9-4782-a65f-783271c9c447']  \n",
       "4  ['83a09c6b-5f2f-421f-ae50-b38acca7e008', '9ff5...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories = pd.read_csv('data/stories.csv')\n",
    "stories.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning\n",
    "We observe that some rows have empty bodies, but they are not `NaN` in a usual way,\n",
    "so we check what character they are and attempt to delete these rows using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' ', ' ')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories['body'][0], stories['body'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting the empty rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello and welcome to BBC News a woman who gave...</td>\n",
       "      <td>['83a09c6b-5f2f-421f-ae50-b38acca7e008', '9ff5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news now out of North Hollywood. A 14 yearold ...</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homelessness his city's greatest failure. That...</td>\n",
       "      <td>['83a09c6b-5f2f-421f-ae50-b38acca7e008', '74e2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Minneapolis police officer Kim Potter guilty o...</td>\n",
       "      <td>['83a09c6b-5f2f-421f-ae50-b38acca7e008', '9ff5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Judy an update now to the wildfires that wiped...</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913', '9a06...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body   \n",
       "0  hello and welcome to BBC News a woman who gave...  \\\n",
       "1  news now out of North Hollywood. A 14 yearold ...   \n",
       "2  homelessness his city's greatest failure. That...   \n",
       "3  Minneapolis police officer Kim Potter guilty o...   \n",
       "4  Judy an update now to the wildfires that wiped...   \n",
       "\n",
       "                                               topic  \n",
       "0  ['83a09c6b-5f2f-421f-ae50-b38acca7e008', '9ff5...  \n",
       "1           ['9ff54ded-904b-4e0c-85ce-a3617f5cb913']  \n",
       "2  ['83a09c6b-5f2f-421f-ae50-b38acca7e008', '74e2...  \n",
       "3  ['83a09c6b-5f2f-421f-ae50-b38acca7e008', '9ff5...  \n",
       "4  ['9ff54ded-904b-4e0c-85ce-a3617f5cb913', '9a06...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories = stories[stories['body'] != ' '].reset_index().drop(['index'], axis=1)\n",
    "stories.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting all topics\n",
    "Our data is now clean! We will move on with the task. First we will see how many unique topics we have.\n",
    "We observe that the data in the `topic` column is made of strings of lists of strings (!) and we will read\n",
    "them using the `literal_eval` from python's built-in `ast` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = set()\n",
    "for i in stories['topic']:\n",
    "    current_topics = ast.literal_eval(i)\n",
    "    for topic in current_topics:\n",
    "        if topic not in topics:\n",
    "            topics.add(topic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the topics now, but they are in a very strange format. We will turn them\n",
    "in the usual numeric format, while also keeping the reverse of them for adding them\n",
    "later on to the dataframe in the same format as the `stories` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_label = {topic: label for label, topic in enumerate(topics)}\n",
    "label_to_topic = {label: topic for label, topic in enumerate(topics)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the `labels` column\n",
    "We add this column in a multilabel-encoded way, while also keeping it in string format\n",
    "for later usage in the colab notebook by `ast.literal_eval` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(row: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Function for generating multilabels-encodings from the `topic` column\n",
    "    in the `stories` dataframe.\n",
    "\n",
    "    Args:\n",
    "        row: a row of a pandas dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        a stringified multilabels-endcoding.\n",
    "    \"\"\"\n",
    "    current_topics = ast.literal_eval(row['topic'])\n",
    "\n",
    "    labels = np.zeros(len(topics), dtype=int)\n",
    "    np.put(labels, [topic_to_label[topic] for topic in current_topics], [1] * len(current_topics))\n",
    "\n",
    "    output_in_str_format = '[' + str(labels[0])\n",
    "    for num in labels[1:]:\n",
    "        output_in_str_format += ', ' + str(num)\n",
    "    \n",
    "    return output_in_str_format + ']'\n",
    "\n",
    "\n",
    "stories['labels'] = stories.apply(generate_labels, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had saved the dataset for exporting to the other notebook for BERT training, which is now unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories.drop(['topic'], axis=1).to_csv('stories_processed.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had also saved the processed `to_fill` dataframe for usage in the BERT notebook on colab, which is\n",
    "now also unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('to_fill_proccessed.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>topic</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello and welcome to BBC News a woman who gave...</td>\n",
       "      <td>['83a09c6b-5f2f-421f-ae50-b38acca7e008', '9ff5...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news now out of North Hollywood. A 14 yearold ...</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homelessness his city's greatest failure. That...</td>\n",
       "      <td>['83a09c6b-5f2f-421f-ae50-b38acca7e008', '74e2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Minneapolis police officer Kim Potter guilty o...</td>\n",
       "      <td>['83a09c6b-5f2f-421f-ae50-b38acca7e008', '9ff5...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Judy an update now to the wildfires that wiped...</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913', '9a06...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body   \n",
       "0  hello and welcome to BBC News a woman who gave...  \\\n",
       "1  news now out of North Hollywood. A 14 yearold ...   \n",
       "2  homelessness his city's greatest failure. That...   \n",
       "3  Minneapolis police officer Kim Potter guilty o...   \n",
       "4  Judy an update now to the wildfires that wiped...   \n",
       "\n",
       "                                               topic   \n",
       "0  ['83a09c6b-5f2f-421f-ae50-b38acca7e008', '9ff5...  \\\n",
       "1           ['9ff54ded-904b-4e0c-85ce-a3617f5cb913']   \n",
       "2  ['83a09c6b-5f2f-421f-ae50-b38acca7e008', '74e2...   \n",
       "3  ['83a09c6b-5f2f-421f-ae50-b38acca7e008', '9ff5...   \n",
       "4  ['9ff54ded-904b-4e0c-85ce-a3617f5cb913', '9a06...   \n",
       "\n",
       "                                          labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the dataset for training a RandomForestClassifier\n",
    "Note: We need to do a seperate vectorization on the split dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stories['body']\n",
    "y = [ast.literal_eval(label) for label in stories['labels'].values]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85, random_state=42)  # setting random state for reproducibility\n",
    "\n",
    "vectorizer_whole = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "vectorizer_train = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "X = vectorizer_whole.fit_transform(X)\n",
    "\n",
    "X_train = vectorizer_train.fit_transform(X_train)\n",
    "X_test = vectorizer_train.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimized hyperparameters\n",
    "This is done by maximizing the mean `Cross Validation` score using `Optuna`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-12 21:52:24,711]\u001b[0m A new study created in memory with name: no-name-a9307132-0dc3-464b-a27b-57dc063db562\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:52:33,611]\u001b[0m Trial 0 finished with value: 0.054540874891433534 and parameters: {'rf_num_estimators': 320, 'rf_max_depth': 10, 'rf_min_samples_split': 11}. Best is trial 0 with value: 0.054540874891433534.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:52:39,559]\u001b[0m Trial 1 finished with value: 0.06832370219148874 and parameters: {'rf_num_estimators': 300, 'rf_max_depth': 11, 'rf_min_samples_split': 29}. Best is trial 1 with value: 0.06832370219148874.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:52:46,480]\u001b[0m Trial 2 finished with value: 0.04580617618017366 and parameters: {'rf_num_estimators': 463, 'rf_max_depth': 9, 'rf_min_samples_split': 27}. Best is trial 1 with value: 0.06832370219148874.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:52:52,003]\u001b[0m Trial 3 finished with value: 0.03862400894612504 and parameters: {'rf_num_estimators': 338, 'rf_max_depth': 8, 'rf_min_samples_split': 27}. Best is trial 1 with value: 0.06832370219148874.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:52:59,169]\u001b[0m Trial 4 finished with value: 0.05784041391196574 and parameters: {'rf_num_estimators': 390, 'rf_max_depth': 10, 'rf_min_samples_split': 12}. Best is trial 1 with value: 0.06832370219148874.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:53:10,099]\u001b[0m Trial 5 finished with value: 0.09433422627341143 and parameters: {'rf_num_estimators': 490, 'rf_max_depth': 14, 'rf_min_samples_split': 26}. Best is trial 5 with value: 0.09433422627341143.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:53:20,729]\u001b[0m Trial 6 finished with value: 0.10811660156634029 and parameters: {'rf_num_estimators': 475, 'rf_max_depth': 15, 'rf_min_samples_split': 19}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:53:27,973]\u001b[0m Trial 7 finished with value: 0.08113403615921409 and parameters: {'rf_num_estimators': 311, 'rf_max_depth': 13, 'rf_min_samples_split': 14}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:53:35,096]\u001b[0m Trial 8 finished with value: 0.08909218662741436 and parameters: {'rf_num_estimators': 320, 'rf_max_depth': 14, 'rf_min_samples_split': 24}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:53:43,281]\u001b[0m Trial 9 finished with value: 0.0824928825827868 and parameters: {'rf_num_estimators': 388, 'rf_max_depth': 13, 'rf_min_samples_split': 14}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:53:47,999]\u001b[0m Trial 10 finished with value: 0.008732099670283402 and parameters: {'rf_num_estimators': 424, 'rf_max_depth': 5, 'rf_min_samples_split': 19}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:53:59,283]\u001b[0m Trial 11 finished with value: 0.1065635050802211 and parameters: {'rf_num_estimators': 498, 'rf_max_depth': 15, 'rf_min_samples_split': 20}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:54:10,184]\u001b[0m Trial 12 finished with value: 0.10345742510976429 and parameters: {'rf_num_estimators': 494, 'rf_max_depth': 15, 'rf_min_samples_split': 20}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:54:19,711]\u001b[0m Trial 13 finished with value: 0.10190353761117397 and parameters: {'rf_num_estimators': 424, 'rf_max_depth': 15, 'rf_min_samples_split': 17}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:54:28,209]\u001b[0m Trial 14 finished with value: 0.07337047475890505 and parameters: {'rf_num_estimators': 460, 'rf_max_depth': 12, 'rf_min_samples_split': 23}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:54:34,522]\u001b[0m Trial 15 finished with value: 0.027947939627216162 and parameters: {'rf_num_estimators': 463, 'rf_max_depth': 7, 'rf_min_samples_split': 17}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:54:43,797]\u001b[0m Trial 16 finished with value: 0.07317554668566904 and parameters: {'rf_num_estimators': 500, 'rf_max_depth': 12, 'rf_min_samples_split': 22}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:54:53,509]\u001b[0m Trial 17 finished with value: 0.10423340834391596 and parameters: {'rf_num_estimators': 437, 'rf_max_depth': 15, 'rf_min_samples_split': 17}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:55:00,626]\u001b[0m Trial 18 finished with value: 0.0852115924459665 and parameters: {'rf_num_estimators': 361, 'rf_max_depth': 13, 'rf_min_samples_split': 21}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:55:11,088]\u001b[0m Trial 19 finished with value: 0.10054638621432505 and parameters: {'rf_num_estimators': 479, 'rf_max_depth': 14, 'rf_min_samples_split': 19}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:55:19,677]\u001b[0m Trial 20 finished with value: 0.07201128932998757 and parameters: {'rf_num_estimators': 440, 'rf_max_depth': 12, 'rf_min_samples_split': 15}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:55:29,583]\u001b[0m Trial 21 finished with value: 0.1065635050802211 and parameters: {'rf_num_estimators': 443, 'rf_max_depth': 15, 'rf_min_samples_split': 18}. Best is trial 6 with value: 0.10811660156634029.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:55:40,204]\u001b[0m Trial 22 finished with value: 0.10889258480049195 and parameters: {'rf_num_estimators': 477, 'rf_max_depth': 15, 'rf_min_samples_split': 18}. Best is trial 22 with value: 0.10889258480049195.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:55:50,067]\u001b[0m Trial 23 finished with value: 0.0939461781554448 and parameters: {'rf_num_estimators': 479, 'rf_max_depth': 14, 'rf_min_samples_split': 24}. Best is trial 22 with value: 0.10889258480049195.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:56:00,418]\u001b[0m Trial 24 finished with value: 0.10326238403474669 and parameters: {'rf_num_estimators': 476, 'rf_max_depth': 15, 'rf_min_samples_split': 15}. Best is trial 22 with value: 0.10889258480049195.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:56:08,698]\u001b[0m Trial 25 finished with value: 0.08055162497691938 and parameters: {'rf_num_estimators': 411, 'rf_max_depth': 13, 'rf_min_samples_split': 21}. Best is trial 22 with value: 0.10889258480049195.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:56:13,876]\u001b[0m Trial 26 finished with value: 0.008538640620208017 and parameters: {'rf_num_estimators': 454, 'rf_max_depth': 5, 'rf_min_samples_split': 20}. Best is trial 22 with value: 0.10889258480049195.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:56:24,298]\u001b[0m Trial 27 finished with value: 0.0914222833637195 and parameters: {'rf_num_estimators': 473, 'rf_max_depth': 14, 'rf_min_samples_split': 16}. Best is trial 22 with value: 0.10889258480049195.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:56:32,830]\u001b[0m Trial 28 finished with value: 0.06327625161338293 and parameters: {'rf_num_estimators': 488, 'rf_max_depth': 11, 'rf_min_samples_split': 19}. Best is trial 22 with value: 0.10889258480049195.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:56:41,363]\u001b[0m Trial 29 finished with value: 0.06502246814423276 and parameters: {'rf_num_estimators': 448, 'rf_max_depth': 11, 'rf_min_samples_split': 12}. Best is trial 22 with value: 0.10889258480049195.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:56:49,632]\u001b[0m Trial 30 finished with value: 0.1112220035261076 and parameters: {'rf_num_estimators': 367, 'rf_max_depth': 15, 'rf_min_samples_split': 23}. Best is trial 30 with value: 0.1112220035261076.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:56:57,550]\u001b[0m Trial 31 finished with value: 0.0997693859641391 and parameters: {'rf_num_estimators': 354, 'rf_max_depth': 15, 'rf_min_samples_split': 22}. Best is trial 30 with value: 0.1112220035261076.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:57:05,385]\u001b[0m Trial 32 finished with value: 0.09375170208933514 and parameters: {'rf_num_estimators': 375, 'rf_max_depth': 14, 'rf_min_samples_split': 25}. Best is trial 30 with value: 0.1112220035261076.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:57:13,263]\u001b[0m Trial 33 finished with value: 0.10656384408556585 and parameters: {'rf_num_estimators': 351, 'rf_max_depth': 15, 'rf_min_samples_split': 22}. Best is trial 30 with value: 0.1112220035261076.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:57:20,309]\u001b[0m Trial 34 finished with value: 0.0824936735952579 and parameters: {'rf_num_estimators': 353, 'rf_max_depth': 13, 'rf_min_samples_split': 28}. Best is trial 30 with value: 0.1112220035261076.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:57:25,680]\u001b[0m Trial 35 finished with value: 0.04697065953941829 and parameters: {'rf_num_estimators': 336, 'rf_max_depth': 9, 'rf_min_samples_split': 30}. Best is trial 30 with value: 0.1112220035261076.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:57:34,098]\u001b[0m Trial 36 finished with value: 0.0914222833637195 and parameters: {'rf_num_estimators': 375, 'rf_max_depth': 14, 'rf_min_samples_split': 22}. Best is trial 30 with value: 0.1112220035261076.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:57:42,761]\u001b[0m Trial 37 finished with value: 0.10908683486303845 and parameters: {'rf_num_estimators': 341, 'rf_max_depth': 15, 'rf_min_samples_split': 10}. Best is trial 30 with value: 0.1112220035261076.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:57:47,477]\u001b[0m Trial 38 finished with value: 0.03338377732863336 and parameters: {'rf_num_estimators': 337, 'rf_max_depth': 7, 'rf_min_samples_split': 10}. Best is trial 30 with value: 0.1112220035261076.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:57:54,913]\u001b[0m Trial 39 finished with value: 0.095304346568328 and parameters: {'rf_num_estimators': 320, 'rf_max_depth': 14, 'rf_min_samples_split': 12}. Best is trial 30 with value: 0.1112220035261076.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:58:01,679]\u001b[0m Trial 40 finished with value: 0.056288334441880805 and parameters: {'rf_num_estimators': 390, 'rf_max_depth': 10, 'rf_min_samples_split': 27}. Best is trial 30 with value: 0.1112220035261076.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:58:08,038]\u001b[0m Trial 41 finished with value: 0.10753452938939034 and parameters: {'rf_num_estimators': 347, 'rf_max_depth': 15, 'rf_min_samples_split': 25}. Best is trial 30 with value: 0.1112220035261076.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:58:14,192]\u001b[0m Trial 42 finished with value: 0.11199889077451196 and parameters: {'rf_num_estimators': 304, 'rf_max_depth': 15, 'rf_min_samples_split': 25}. Best is trial 42 with value: 0.11199889077451196.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:58:19,799]\u001b[0m Trial 43 finished with value: 0.08171622133794561 and parameters: {'rf_num_estimators': 301, 'rf_max_depth': 13, 'rf_min_samples_split': 26}. Best is trial 42 with value: 0.11199889077451196.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:58:26,543]\u001b[0m Trial 44 finished with value: 0.09161619442092124 and parameters: {'rf_num_estimators': 329, 'rf_max_depth': 14, 'rf_min_samples_split': 24}. Best is trial 42 with value: 0.11199889077451196.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:58:33,641]\u001b[0m Trial 45 finished with value: 0.10617534396047289 and parameters: {'rf_num_estimators': 307, 'rf_max_depth': 15, 'rf_min_samples_split': 10}. Best is trial 42 with value: 0.11199889077451196.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:58:41,687]\u001b[0m Trial 46 finished with value: 0.10675707713207806 and parameters: {'rf_num_estimators': 365, 'rf_max_depth': 15, 'rf_min_samples_split': 14}. Best is trial 42 with value: 0.11199889077451196.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:58:49,198]\u001b[0m Trial 47 finished with value: 0.08171701235041673 and parameters: {'rf_num_estimators': 403, 'rf_max_depth': 13, 'rf_min_samples_split': 23}. Best is trial 42 with value: 0.11199889077451196.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:58:55,761]\u001b[0m Trial 48 finished with value: 0.09239285566576243 and parameters: {'rf_num_estimators': 327, 'rf_max_depth': 14, 'rf_min_samples_split': 26}. Best is trial 42 with value: 0.11199889077451196.\u001b[0m\n",
      "\u001b[32m[I 2023-05-12 21:59:03,816]\u001b[0m Trial 49 finished with value: 0.07414645799305672 and parameters: {'rf_num_estimators': 421, 'rf_max_depth': 12, 'rf_min_samples_split': 18}. Best is trial 42 with value: 0.11199889077451196.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# This is a function used only by Optuna for optimizing the ML Model Hyperparameters\n",
    "# It is in a typical format for Optuna\n",
    "def objective(trial):\n",
    "    hyperparamaters = {\n",
    "        'n_estimators': trial.suggest_int('rf_num_estimators', 300, 500),\n",
    "        'max_depth': trial.suggest_int('rf_max_depth', 5, 15),\n",
    "        'min_samples_split': trial.suggest_int('rf_min_samples_split', 10, 30),\n",
    "    }\n",
    "\n",
    "    model = RandomForestClassifier(**hyperparamaters)\n",
    "\n",
    "    score = cross_val_score(model, X, y, cv=3, n_jobs=-1)\n",
    "    return score.mean()\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the hyperparameters ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = {\n",
    "    'n_estimators': study.best_params.pop('rf_num_estimators'),\n",
    "    'max_depth': study.best_params.pop('rf_max_depth'),\n",
    "    'min_samples_split': study.best_params.pop('rf_min_samples_split'),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model to the data.\n",
    "We will do this twice, once with the optimized hyperparameters, and\n",
    "once with the vanilla hyperparameters, as there is a chance that the\n",
    "default ones may perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=15, min_samples_split=25, n_estimators=304)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=15, min_samples_split=25, n_estimators=304)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=15, min_samples_split=25, n_estimators=304)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vanilla = RandomForestClassifier()\n",
    "model_optimized = RandomForestClassifier(**best_hyperparameters)\n",
    "model_vanilla.fit(X_train, y_train)\n",
    "model_optimized.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting models` predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vanilla = model_vanilla.predict(X_test)\n",
    "y_pred_optimized = model_optimized.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating models` accuracies\n",
    "This is done by counting the number of true predictions (whether negatives or positives)\n",
    "divided by the number of all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Vanilla RF: 0.9423\n",
      "Accuracy of Optimized RF: 0.9148\n"
     ]
    }
   ],
   "source": [
    "number_of_all_labels = len(y_test) * 15  # as we have 15 labels in each multilabel-encoding\n",
    "\n",
    "accuracy_vanilla = (y_test == y_pred_vanilla).sum() / number_of_all_labels\n",
    "accuracy_optimized = (y_test == y_pred_optimized).sum() / number_of_all_labels\n",
    "\n",
    "print(f'Accuracy of Vanilla RF: {accuracy_vanilla:.4f}')\n",
    "print(f'Accuracy of Optimized RF: {accuracy_optimized:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model on the whole dataset\n",
    "Weirdly enough, we see that the optimized version has a worst performance in accuracy!\n",
    "So we will go with the default RandomForestClassifier settings and train it on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topic(row: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Function for predicting the `topic`s of a body of text.\n",
    "\n",
    "    Args:\n",
    "        row: a row of a pandas dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        a list of weird-formatted topics.\n",
    "    \"\"\"\n",
    "    text = [row['body']]\n",
    "    text = vectorizer_whole.transform(text)\n",
    "\n",
    "    topics = model.predict(text)[0]\n",
    "\n",
    "    topics = str([str(label_to_topic[i]) for i, t in enumerate(topics) if t == 1])\n",
    "\n",
    "    return topics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the `topics` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topics'] = df.apply(predict_topic, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_words</th>\n",
       "      <th>last_words</th>\n",
       "      <th>source_video_id</th>\n",
       "      <th>body</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well knew. This morning police need your help</td>\n",
       "      <td>gunpoint, beating him and stealing his cell ph...</td>\n",
       "      <td>18246</td>\n",
       "      <td>Well knew. This morning police need your help ...</td>\n",
       "      <td>464928</td>\n",
       "      <td>505910</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a call. San Francisco firefighters rescued a man</td>\n",
       "      <td>all the way down to the ocean.</td>\n",
       "      <td>12387</td>\n",
       "      <td>a call. San Francisco firefighters rescued a m...</td>\n",
       "      <td>359020</td>\n",
       "      <td>385526</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paul. Meanwhile, the state set a record in</td>\n",
       "      <td>night through conservation, some 4000 conserva...</td>\n",
       "      <td>16859</td>\n",
       "      <td>Paul. Meanwhile, the state set a record in ene...</td>\n",
       "      <td>60704</td>\n",
       "      <td>101238</td>\n",
       "      <td>['83a09c6b-5f2f-421f-ae50-b38acca7e008']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emergency crews in Florida continue to search for</td>\n",
       "      <td>in Florida to more than 850,000 homes.</td>\n",
       "      <td>18246</td>\n",
       "      <td>Emergency crews in Florida continue to search ...</td>\n",
       "      <td>505290</td>\n",
       "      <td>534958</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But even though the state never ordered rolling</td>\n",
       "      <td>feel since their power got cut out needlessly.</td>\n",
       "      <td>16859</td>\n",
       "      <td>But even though the state never ordered rollin...</td>\n",
       "      <td>100910</td>\n",
       "      <td>283606</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aid. And today, president Joe Biden and first</td>\n",
       "      <td>to view the destruction caused by Hurricane Ian.</td>\n",
       "      <td>18246</td>\n",
       "      <td>aid. And today, president Joe Biden and first ...</td>\n",
       "      <td>546494</td>\n",
       "      <td>593818</td>\n",
       "      <td>['83a09c6b-5f2f-421f-ae50-b38acca7e008']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In the last month, there have been numerous</td>\n",
       "      <td>are necessary to crack down on those hackers.</td>\n",
       "      <td>18246</td>\n",
       "      <td>In the last month, there have been numerous da...</td>\n",
       "      <td>614910</td>\n",
       "      <td>700574</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and the warriors are playing the Boston Celtics</td>\n",
       "      <td>that. We'll see if they get it tonight.</td>\n",
       "      <td>12387</td>\n",
       "      <td>and the warriors are playing the Boston Celtic...</td>\n",
       "      <td>419994</td>\n",
       "      <td>649502</td>\n",
       "      <td>['b49207eb-96eb-4b73-b534-adc0ef85022a']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>And San Leandro police searching for the person</td>\n",
       "      <td>footage to try to piece together more informat...</td>\n",
       "      <td>16859</td>\n",
       "      <td>And San Leandro police searching for the perso...</td>\n",
       "      <td>578612</td>\n",
       "      <td>619638</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The updated Bivalent Coronavirus booster shot ...</td>\n",
       "      <td>on their vaccinations getting severe illness f...</td>\n",
       "      <td>16859</td>\n",
       "      <td>The updated Bivalent Coronavirus booster shot ...</td>\n",
       "      <td>619310</td>\n",
       "      <td>660654</td>\n",
       "      <td>['96326734-fd82-4350-b45c-513e7eb9147c']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>temperatures tonight, only back down to the un...</td>\n",
       "      <td>stuff. Stay hydrated, take breaks. Don't overd...</td>\n",
       "      <td>16859</td>\n",
       "      <td>temperatures tonight, only back down to the un...</td>\n",
       "      <td>1127008</td>\n",
       "      <td>1208198</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>And National Guard teams are also on the</td>\n",
       "      <td>north and South Carolina. Fasttracking federal...</td>\n",
       "      <td>18246</td>\n",
       "      <td>And National Guard teams are also on the groun...</td>\n",
       "      <td>534674</td>\n",
       "      <td>548314</td>\n",
       "      <td>['83a09c6b-5f2f-421f-ae50-b38acca7e008']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A man died after falling from an escalator</td>\n",
       "      <td>name or what led to that fall.</td>\n",
       "      <td>18246</td>\n",
       "      <td>A man died after falling from an escalator aft...</td>\n",
       "      <td>593490</td>\n",
       "      <td>615274</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>All right. Well happening today. Friends and f...</td>\n",
       "      <td>Meanwhile, the suspected killer remains in the...</td>\n",
       "      <td>18246</td>\n",
       "      <td>All right. Well happening today. Friends and f...</td>\n",
       "      <td>218170</td>\n",
       "      <td>345758</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>detectives are searching for six people they say</td>\n",
       "      <td>with any information is asked to call police.</td>\n",
       "      <td>18246</td>\n",
       "      <td>detectives are searching for six people they s...</td>\n",
       "      <td>350830</td>\n",
       "      <td>459442</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>San Jose police say a security guard trying</td>\n",
       "      <td>not released yet by the Santa Clara County</td>\n",
       "      <td>12387</td>\n",
       "      <td>San Jose police say a security guard trying to...</td>\n",
       "      <td>98130</td>\n",
       "      <td>276282</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>San Jose police just announcing today an arrest</td>\n",
       "      <td>not reveal a motive for the deadly shooting.</td>\n",
       "      <td>12387</td>\n",
       "      <td>San Jose police just announcing today an arres...</td>\n",
       "      <td>283804</td>\n",
       "      <td>330978</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>shooting. Surveillance video shows how a car t...</td>\n",
       "      <td>this case, please give them a call.</td>\n",
       "      <td>12387</td>\n",
       "      <td>shooting. Surveillance video shows how a car t...</td>\n",
       "      <td>329682</td>\n",
       "      <td>360102</td>\n",
       "      <td>['9ff54ded-904b-4e0c-85ce-a3617f5cb913']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          first_words   \n",
       "0       Well knew. This morning police need your help  \\\n",
       "1    a call. San Francisco firefighters rescued a man   \n",
       "2          Paul. Meanwhile, the state set a record in   \n",
       "3   Emergency crews in Florida continue to search for   \n",
       "4     But even though the state never ordered rolling   \n",
       "5       aid. And today, president Joe Biden and first   \n",
       "6         In the last month, there have been numerous   \n",
       "7     and the warriors are playing the Boston Celtics   \n",
       "8     And San Leandro police searching for the person   \n",
       "9   The updated Bivalent Coronavirus booster shot ...   \n",
       "10  temperatures tonight, only back down to the un...   \n",
       "11           And National Guard teams are also on the   \n",
       "12         A man died after falling from an escalator   \n",
       "13  All right. Well happening today. Friends and f...   \n",
       "14   detectives are searching for six people they say   \n",
       "15        San Jose police say a security guard trying   \n",
       "16    San Jose police just announcing today an arrest   \n",
       "17  shooting. Surveillance video shows how a car t...   \n",
       "\n",
       "                                           last_words  source_video_id   \n",
       "0   gunpoint, beating him and stealing his cell ph...            18246  \\\n",
       "1                      all the way down to the ocean.            12387   \n",
       "2   night through conservation, some 4000 conserva...            16859   \n",
       "3              in Florida to more than 850,000 homes.            18246   \n",
       "4      feel since their power got cut out needlessly.            16859   \n",
       "5    to view the destruction caused by Hurricane Ian.            18246   \n",
       "6       are necessary to crack down on those hackers.            18246   \n",
       "7             that. We'll see if they get it tonight.            12387   \n",
       "8   footage to try to piece together more informat...            16859   \n",
       "9   on their vaccinations getting severe illness f...            16859   \n",
       "10  stuff. Stay hydrated, take breaks. Don't overd...            16859   \n",
       "11  north and South Carolina. Fasttracking federal...            18246   \n",
       "12                     name or what led to that fall.            18246   \n",
       "13  Meanwhile, the suspected killer remains in the...            18246   \n",
       "14      with any information is asked to call police.            18246   \n",
       "15         not released yet by the Santa Clara County            12387   \n",
       "16       not reveal a motive for the deadly shooting.            12387   \n",
       "17                this case, please give them a call.            12387   \n",
       "\n",
       "                                                 body    start      end   \n",
       "0   Well knew. This morning police need your help ...   464928   505910  \\\n",
       "1   a call. San Francisco firefighters rescued a m...   359020   385526   \n",
       "2   Paul. Meanwhile, the state set a record in ene...    60704   101238   \n",
       "3   Emergency crews in Florida continue to search ...   505290   534958   \n",
       "4   But even though the state never ordered rollin...   100910   283606   \n",
       "5   aid. And today, president Joe Biden and first ...   546494   593818   \n",
       "6   In the last month, there have been numerous da...   614910   700574   \n",
       "7   and the warriors are playing the Boston Celtic...   419994   649502   \n",
       "8   And San Leandro police searching for the perso...   578612   619638   \n",
       "9   The updated Bivalent Coronavirus booster shot ...   619310   660654   \n",
       "10  temperatures tonight, only back down to the un...  1127008  1208198   \n",
       "11  And National Guard teams are also on the groun...   534674   548314   \n",
       "12  A man died after falling from an escalator aft...   593490   615274   \n",
       "13  All right. Well happening today. Friends and f...   218170   345758   \n",
       "14  detectives are searching for six people they s...   350830   459442   \n",
       "15  San Jose police say a security guard trying to...    98130   276282   \n",
       "16  San Jose police just announcing today an arres...   283804   330978   \n",
       "17  shooting. Surveillance video shows how a car t...   329682   360102   \n",
       "\n",
       "                                      topics  \n",
       "0   ['9ff54ded-904b-4e0c-85ce-a3617f5cb913']  \n",
       "1   ['9ff54ded-904b-4e0c-85ce-a3617f5cb913']  \n",
       "2   ['83a09c6b-5f2f-421f-ae50-b38acca7e008']  \n",
       "3   ['9ff54ded-904b-4e0c-85ce-a3617f5cb913']  \n",
       "4                                         []  \n",
       "5   ['83a09c6b-5f2f-421f-ae50-b38acca7e008']  \n",
       "6                                         []  \n",
       "7   ['b49207eb-96eb-4b73-b534-adc0ef85022a']  \n",
       "8   ['9ff54ded-904b-4e0c-85ce-a3617f5cb913']  \n",
       "9   ['96326734-fd82-4350-b45c-513e7eb9147c']  \n",
       "10                                        []  \n",
       "11  ['83a09c6b-5f2f-421f-ae50-b38acca7e008']  \n",
       "12  ['9ff54ded-904b-4e0c-85ce-a3617f5cb913']  \n",
       "13  ['9ff54ded-904b-4e0c-85ce-a3617f5cb913']  \n",
       "14  ['9ff54ded-904b-4e0c-85ce-a3617f5cb913']  \n",
       "15  ['9ff54ded-904b-4e0c-85ce-a3617f5cb913']  \n",
       "16  ['9ff54ded-904b-4e0c-85ce-a3617f5cb913']  \n",
       "17  ['9ff54ded-904b-4e0c-85ce-a3617f5cb913']  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('to_fill_finalized.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
